{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc858f19-8b1f-4f70-a8c3-096e3c73f864",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from yolov3_pytorch.yolov3_tiny import Yolov3Tiny\n",
    "from finn.util.visualization import showInNetron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1bba98b-3464-46c7-97a8-a09bae140060",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hao/finn/notebooks/yolov3_pytorch/yolov3_pytorch/yolov3_base.py:26: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert shape[1] == 3 and shape[2] % 32 == 0 and shape[3] % 32 == 0, f\"Tensor shape should be [bs, 3, x*32, y*32], was {shape}\"\n",
      "/usr/local/lib/python3.10/dist-packages/torch/onnx/_internal/jit_utils.py:258: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at ../torch/csrc/jit/passes/onnx/constant_fold.cpp:179.)\n",
      "  _C._jit_pass_onnx_node_shape_type_inference(node, params_dict, opset_version)\n",
      "/usr/local/lib/python3.10/dist-packages/torch/onnx/_internal/jit_utils.py:258: UserWarning: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (Triggered internally at ../torch/csrc/jit/passes/onnx/shape_type_inference.cpp:1884.)\n",
      "  _C._jit_pass_onnx_node_shape_type_inference(node, params_dict, opset_version)\n",
      "/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py:687: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at ../torch/csrc/jit/passes/onnx/constant_fold.cpp:179.)\n",
      "  _C._jit_pass_onnx_graph_shape_type_inference(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py:687: UserWarning: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (Triggered internally at ../torch/csrc/jit/passes/onnx/shape_type_inference.cpp:1884.)\n",
      "  _C._jit_pass_onnx_graph_shape_type_inference(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py:1178: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at ../torch/csrc/jit/passes/onnx/constant_fold.cpp:179.)\n",
      "  _C._jit_pass_onnx_graph_shape_type_inference(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py:1178: UserWarning: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (Triggered internally at ../torch/csrc/jit/passes/onnx/shape_type_inference.cpp:1884.)\n",
      "  _C._jit_pass_onnx_graph_shape_type_inference(\n"
     ]
    }
   ],
   "source": [
    "model = Yolov3Tiny(80)\n",
    "model.load_state_dict(torch.load(\"yolov3_tiny_coco_01.h5\"))\n",
    "\n",
    "dummy_input = torch.randn(1, 3, 416, 416)\n",
    "result = model(dummy_input)\n",
    "torch.onnx.export(model, dummy_input, \"model.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20143b0a-56bc-42a8-abeb-ccea3f923d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving 'model.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f10328f7310>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "showInNetron(\"model.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fbefec2-1594-4d79-a4fb-e127ebc4abbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import brevitas.nn as qnn\n",
    "from brevitas.core.quant import QuantType\n",
    "from brevitas.export import export_qonnx\n",
    "\n",
    "def convert_to_quantized_layer(layer, weight_bit_width=8):\n",
    "    # This function should be expanded to handle different types of layers and configurations\n",
    "    quant_layer = None\n",
    "    if isinstance(layer, torch.nn.Linear):\n",
    "        quant_layer = qnn.QuantLinear(\n",
    "            in_features=layer.in_features, \n",
    "            out_features=layer.out_features, \n",
    "            bias=layer.bias is not None,\n",
    "            weight_bit_width=weight_bit_width)\n",
    "    elif isinstance(layer, torch.nn.Conv2d):\n",
    "        quant_layer = qnn.QuantConv2d(\n",
    "            in_channels=layer.in_channels,\n",
    "            out_channels=layer.out_channels,\n",
    "            kernel_size=layer.kernel_size,\n",
    "            stride=layer.stride,\n",
    "            padding=layer.padding,\n",
    "            dilation=layer.dilation,\n",
    "            groups=layer.groups,\n",
    "            bias=layer.bias is not None,\n",
    "            weight_quant_type=QuantType.INT,  # Weight quantization type\n",
    "            weight_bit_width=weight_bit_width  # Bit width for the weights\n",
    "        )\n",
    "    elif isinstance(layer, torch.nn.LeakyReLU):\n",
    "        quant_layer = qnn.QuantReLU(\n",
    "            bit_width=weight_bit_width,  # Bit width for the activation\n",
    "            quant_type=QuantType.INT  # Activation quantization type\n",
    "        )\n",
    "    elif isinstance(layer, torch.nn.Upsample):\n",
    "        quant_layer = QuantUpsample(\n",
    "            size=layer.size, \n",
    "            scale_factor=layer.scale_factor, \n",
    "            mode=layer.mode, \n",
    "            align_corners=layer.align_corners\n",
    "        )\n",
    "    # Add other layer conversions here\n",
    "    # Note: Batch normalization layers might not need to be quantized in many cases.\n",
    "    # elif isinstance(layer, torch.nn.BatchNorm2d):\n",
    "    else:\n",
    "        quant_layer = layer  # Return the original layer if no conversion is defined\n",
    "\n",
    "    if hasattr(layer, 'weight'):\n",
    "        quant_layer.weight.data = layer.weight.data\n",
    "    return quant_layer\n",
    "\n",
    "def quantize_model(model):\n",
    "    for name, module in model.named_children():\n",
    "        model._modules[name] = convert_to_quantized_layer(module)\n",
    "        quantize_model(model._modules[name])  # Recursively apply to submodules\n",
    "    return model\n",
    "\n",
    "# Example usage\n",
    "quantized_model = quantize_model(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834429dd-1b67-4d15-a489-dd224801cae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_qonnx(quantized_model, export_path=\"quant_model.onnx\", input_shape=dummy_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80f1ebb2-5ed3-4878-93ab-253ef10b0c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8081\n",
      "Serving 'quant_model.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f10327551b0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "showInNetron(\"quant_model.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7f94e8-569a-4e9c-807d-974cdb11da50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def test(model, test_loader):    \n",
    "    # ensure model is in eval mode\n",
    "    model.eval() \n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "   \n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            inputs, target = data\n",
    "            inputs, target = inputs.to(device), target.to(device)\n",
    "            output_orig = model(inputs.float())\n",
    "            # run the output through sigmoid\n",
    "            output = torch.sigmoid(output_orig)  \n",
    "            # compare against a threshold of 0.5 to generate 0/1\n",
    "            pred = (output.detach().cpu().numpy() > 0.5) * 1\n",
    "            target = target.cpu().float()\n",
    "            y_true.extend(target.tolist()) \n",
    "            y_pred.extend(pred.reshape(-1).tolist())\n",
    "        \n",
    "    return accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0a68a1c-7cf4-40b2-8443-17ca24dfcd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "anno_json = '../coco/annotations/instances_val2017.json'\n",
    "img_path = '../coco/images/val2017'\n",
    "with open(anno_json) as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c94c5ac-2d70-4bb1-9b31-b44a82a57734",
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_fname(idx):\n",
    "    return f\"{img_path}/{idx:012d}.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16c122d5-3906-4b53-baee-fa2007df533a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_all(data_imgs, sz=416, conf_thresh=.2, nms_thresh=.4):\n",
    "    results = []\n",
    "    img_ids = []\n",
    "    for dat in tqdm(data_imgs):\n",
    "        fname = dat['file_name']\n",
    "        f_id = dat['id']\n",
    "        img_ids.append(f_id)\n",
    "        print(img_fname(f_id))\n",
    "        img = cv2.imread(img_fname(f_id), cv2.IMREAD_COLOR)\n",
    "\n",
    "        if sz:\n",
    "            #img = img.resize((np.array(img), (sz, sz), interpolation=cv2.INTER_AREA)\n",
    "            # img = img.resize((sz, sz))\n",
    "            img = cv2.resize(img, (sz, sz))\n",
    "        img = img.transpose((2, 0, 1)).reshape(1, 3, sz, sz)\n",
    "        # img = np.ascontiguousarray(img)\n",
    "        img = img.astype(np.uint8)\n",
    "\n",
    "        print(img)\n",
    "        \n",
    "        # img_torch = torch.from_numpy(img).cuda()\n",
    "        img_torch = torch.from_numpy(img)\n",
    "        \n",
    "        all_boxes = quantized_model.predict_img(img_torch, conf_thresh=conf_thresh)[0]\n",
    "        boxes = nms(all_boxes, nms_thresh=nms_thresh)\n",
    "\n",
    "        width = dat['width']\n",
    "        height = dat['height']\n",
    "\n",
    "        for pred in boxes:\n",
    "            box = np.array(pred[:4])\n",
    "            box[:2] -= box[2:4]/2\n",
    "            # box[2:4] = box[2:4]/2 + box[:2]\n",
    "            x,w = box[0]*dat['width'], box[2]*dat['width']\n",
    "            y,h = box[1]*dat['height'], box[3]*dat['height']\n",
    "            cat = class_conversion[int(pred[-1])]\n",
    "            res = {\"image_id\":f_id, \"category_id\":cat,\n",
    "                    \"bbox\":[x, y, w, h], \"score\": pred[-2]}\n",
    "\n",
    "            results.append(res)\n",
    "    \n",
    "    print(f\"Results total {len(results)}. N of files {len(img_ids)}\")\n",
    "    return results, img_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c67682c-a370-4c0a-9c79-af7faf8671e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "results, img_ids = predict_all(data['images'], conf_thresh=.2, nms_thresh=.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0437dec2-00c5-4d6d-ac80-84325b58e771",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

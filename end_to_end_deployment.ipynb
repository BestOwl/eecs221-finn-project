{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74914da6-6d87-4783-ad10-ea9c9c1b236f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx \n",
    "import torch \n",
    "\n",
    "import brevitas.onnx as bo\n",
    "\n",
    "import inspect\n",
    "import netron\n",
    "from IPython.display import IFrame\n",
    "\n",
    "def showSrc(what):\n",
    "    print(\"\".join(inspect.getsourcelines(what)[0]))\n",
    "\n",
    "def showInNetron(model_filename):\n",
    "    netron.start(model_filename, address=(\"0.0.0.0\", 8081))\n",
    "    return IFrame(src=\"http://0.0.0.0:8081/\", width=\"100%\", height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63be0b63-cbfd-4d74-b253-2fed1a2acbd5",
   "metadata": {},
   "source": [
    "The exported ONNX model (from Brevitas) must be given with the following lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3491d568-f6d8-46a6-8a07-02afe02cae0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hao/finn/deps/qonnx/src/qonnx/core/modelwrapper.py:93: UserWarning: Some old-style domain attributes were automatically converted to new-style,\n",
      "                i.e. domain=finn to domain=qonnx.custom_op.<general|fpgadataflow|...>\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from qonnx.core.modelwrapper import ModelWrapper\n",
    "\n",
    "build_dir = \"./yolo/\"\n",
    "\n",
    "model_file_path = build_dir + \"/best.finn.onnx\"\n",
    "model_for_sim = ModelWrapper(model_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bfcd3c-7e92-4a71-b7d6-29c8aaab32be",
   "metadata": {},
   "source": [
    "Graph based transformations and optimizations are applied to the ONNX model and the final model is saved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ea7dbd5-5471-4a73-b94b-96c1b3089532",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qonnx.transformation.general import GiveReadableTensorNames, GiveUniqueNodeNames, RemoveStaticGraphInputs\n",
    "from qonnx.transformation.infer_shapes import InferShapes\n",
    "from qonnx.transformation.infer_datatypes import InferDataTypes\n",
    "from qonnx.transformation.fold_constants import FoldConstants\n",
    "\n",
    "model_for_sim = model_for_sim.transform(InferShapes())\n",
    "model_for_sim = model_for_sim.transform(FoldConstants())\n",
    "model_for_sim = model_for_sim.transform(GiveUniqueNodeNames())\n",
    "model_for_sim = model_for_sim.transform(GiveReadableTensorNames())\n",
    "model_for_sim = model_for_sim.transform(InferDataTypes())\n",
    "model_for_sim = model_for_sim.transform(RemoveStaticGraphInputs())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6310c599-65cc-44c8-bd69-9192170b0964",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hao/finn/deps/qonnx/src/qonnx/transformation/merge_onnx_models.py:70: UserWarning: [MergeONNXModels] opsets for models to merge differ: 14 vs 9, output model will use opset 14\n",
      "  warnings.warn(\n",
      "/home/hao/finn/deps/qonnx/src/qonnx/transformation/infer_data_layouts.py:127: UserWarning: Assuming 4D input is NCHW\n",
      "  warnings.warn(\"Assuming 4D input is NCHW\")\n"
     ]
    }
   ],
   "source": [
    "from finn.util.pytorch import ToTensor, NormalizePreProc\n",
    "from qonnx.transformation.merge_onnx_models import MergeONNXModels\n",
    "from qonnx.core.datatype import DataType\n",
    "\n",
    "global_inp_name = model_for_sim.graph.input[0].name\n",
    "\n",
    "ishape = model_for_sim.get_tensor_shape(global_inp_name)\n",
    "\n",
    " # preprocessing: torchvision's ToTensor divides uint8 inputs by 255\n",
    "totensor_pyt = ToTensor() #preproc_pyt = NormalizePreProc(mean = [0.485, 0.456, 0.406], std = 0.226, channels = 3)\n",
    "#preproc_pyt = NormalizePreProc(mean = [0.485, 0.456, 0.406], std = 1, channels = 3)\n",
    "\n",
    "\n",
    "chkpt_preproc_name = build_dir+\"/yolo_pre_post_tidy.onnx\"\n",
    "bo.export_qonnx(totensor_pyt, export_path=chkpt_preproc_name, input_shape=ishape) #(preproc_pyt, ishape, chkpt_preproc_name)\n",
    "\n",
    " # join preprocessing and core model\n",
    "pre_model = ModelWrapper(chkpt_preproc_name)\n",
    "model_for_sim = model_for_sim.transform(MergeONNXModels(pre_model))\n",
    "\n",
    "\n",
    " # add input quantization annotation: UINT8 for all BNN-PYNQ models\n",
    "global_inp_name = model_for_sim.graph.input[0].name\n",
    "model_for_sim.set_tensor_datatype(global_inp_name, DataType[\"UINT8\"])\n",
    "\n",
    "model_for_sim.save(build_dir+\"/yolo_pre_post_tidy.onnx\")\n",
    "#showInNetron(build_dir+\"/yolo_pre_post_tidy.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b5875ea-2964-4f55-ace7-32f0a6ac9336",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_for_sim = model_for_sim.transform(InferShapes())\n",
    "model_for_sim = model_for_sim.transform(FoldConstants())\n",
    "model_for_sim = model_for_sim.transform(GiveUniqueNodeNames())\n",
    "model_for_sim = model_for_sim.transform(GiveReadableTensorNames())\n",
    "model_for_sim = model_for_sim.transform(InferDataTypes())\n",
    "model_for_sim = model_for_sim.transform(RemoveStaticGraphInputs())\n",
    "\n",
    "model_for_sim.save(build_dir+\"/yolo_pre_post_tidy.onnx\")\n",
    "#showInNetron(build_dir+\"/yolo_pre_post_tidy.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc331006-08a3-4f62-a0aa-57e7ffa6cfbb",
   "metadata": {},
   "source": [
    "Now, the model is ready for bitstream synthesis!\n",
    "\n",
    "Before the synthesis, “folding_config.json” should be created for resource allocation and parallelism. The parameters on the file can be changed to get lower latency and higher throughput. This file should be like this:\n",
    "\n",
    "With the following lines, .bit and .hwh files are synthesized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f859647-ad18-4d28-9d39-bbcf71a7031a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building dataflow accelerator from ./yolo//yolo_pre_post_tidy.onnx\n",
      "Intermediate outputs will be generated in /tmp/finn_dev_hao\n",
      "Final outputs will be generated in ./yolo//output\n",
      "Build log is at ./yolo//output/build_dataflow.log\n",
      "Running step: step_qonnx_to_finn [1/18]\n",
      "Running step: step_tidy_up [2/18]\n",
      "Running step: step_streamline [3/18]\n",
      "Running step: step_convert_to_hls [4/18]\n",
      "Running step: step_create_dataflow_partition [5/18]\n",
      "Running step: step_target_fps_parallelization [6/18]\n",
      "Running step: step_apply_folding_config [7/18]\n",
      "Running step: step_minimize_bit_width [8/18]\n",
      "Running step: step_generate_estimate_reports [9/18]\n",
      "Running step: step_hls_codegen [10/18]\n",
      "Running step: step_hls_ipgen [11/18]\n",
      "Running step: step_set_fifo_depths [12/18]\n",
      "Running step: step_create_stitched_ip [13/18]\n",
      "Running step: step_measure_rtlsim_performance [14/18]\n",
      "Running step: step_out_of_context_synthesis [15/18]\n",
      "Running step: step_synthesize_bitfile [16/18]\n",
      "Running step: step_make_pynq_driver [17/18]\n",
      "Running step: step_deployment_package [18/18]\n",
      "Completed successfully\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import finn.builder.build_dataflow as build\n",
    "import finn.builder.build_dataflow_config as build_cfg\n",
    "#import os\n",
    "#import shutil\n",
    "\n",
    "build_dir = \"./yolo/\"\n",
    "\n",
    "model_file = build_dir + \"/yolo_pre_post_tidy.onnx\"\n",
    "\n",
    "final_output_dir = build_dir + \"/output\"\n",
    "\n",
    "cfg = build.DataflowBuildConfig(\n",
    "    output_dir          = final_output_dir,\n",
    "    #mvau_wwidth_max     = 80,\n",
    "    folding_config_file = build_dir + \"/folding_config.json\",\n",
    "    auto_fifo_depths    = False,\n",
    "    #large_fifo_mem_style = 'auto',\n",
    "    target_fps          = 100000,\n",
    "    synth_clk_period_ns = 10.0,\n",
    "    board               = \"KV260_SOM\",\n",
    "    #fpga_part           = \"xc7z020clg400-1\",\n",
    "    shell_flow_type     = build_cfg.ShellFlowType.VIVADO_ZYNQ,\n",
    "    generate_outputs=[\n",
    "        build_cfg.DataflowOutputType.ESTIMATE_REPORTS,\n",
    "        build_cfg.DataflowOutputType.BITFILE,\n",
    "        build_cfg.DataflowOutputType.PYNQ_DRIVER,\n",
    "        build_cfg.DataflowOutputType.DEPLOYMENT_PACKAGE,\n",
    "    ]\n",
    ")\n",
    "\n",
    "build.build_dataflow_cfg(model_file, cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9aa3eb3-58c0-48df-b00b-75356d82e4d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
